{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "net_soft_FL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu03efI6MbdL"
      },
      "source": [
        "This is a read-only notebook. To run this, you can first make a copy of it, and then run the new notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44FFES-r6y0"
      },
      "source": [
        "# Federated Learning for Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbNz2tuvsAFB"
      },
      "source": [
        "This python notebook includes the code for the Network Softwarization final project.\n",
        "\n",
        "In this project, we use Federated Learning to build a model for text generation. First, we load a pre-trained Keras model, and then, refine it using federated training. The reasons for taking this approach are thoroughly explained in the report.\n",
        "\n",
        "For the pre-trained model, we use the text from two of the Charles Dickens' books, and for the federated learning part, we use a federated version of works of Shakespeare provided by TFF.\n",
        "\n",
        "This project is based on TensorFlow's federated learning tutorial for text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pEgERqgR7u_"
      },
      "source": [
        "## Install and test `tensorflow_federated`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0GwNWJc-8a"
      },
      "source": [
        "Install `tensorflow_federated` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LcC1AwjoqfR",
        "outputId": "89fb925e-8ddc-47f5-c142-0cb79abee3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow_federated"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 430kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 1.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 37.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 40kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 38.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 42.1MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtzjmViOSQAq"
      },
      "source": [
        "Import necessary packages and test to see if `tff` is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjDQysatrc2S",
        "outputId": "905be06c-e225-401d-821f-7998969589e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "np.random.seed(0)\n",
        "\n",
        "# Test the TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyICXwVAxvW9"
      },
      "source": [
        "## Load the pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC2A0EQCfFnh"
      },
      "source": [
        "First, we start with an RNN model that generates ASCII characters, and then we will refine it via federated learning. This model is previously trained in one of TensorFlow tutorials ([Text generation with an RNN](https://www.tensorflow.org/tutorials/sequences/text_generation)).\n",
        "\n",
        "In order to use the works of Shakespeare for federated learning step, the model is pre-trained on the text from the Charles Dickens'\n",
        "    [A Tale of Two Cities](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt)\n",
        "    and\n",
        "    [A Christmas Carol](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt),\n",
        " and the final model was saved with `tf.keras.models.save_model(include_optimizer=False)`.\n",
        "   \n",
        "After this step, we will use federated learning to fine-tune this model for Shakespeare, using a federated version of the data provided by TFF in `tff.simulation.datasets.shakespeare.load_data()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgF8e2Ksyq1F"
      },
      "source": [
        "Generate the vocab lookup tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR"
      },
      "source": [
        "# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens:\n",
        "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EH6MFRdzAwd"
      },
      "source": [
        "Load the pre-trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIK674SrtCTm"
      },
      "source": [
        "def load_model(batch_size):\n",
        "  urls = {\n",
        "      1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n",
        "      8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
        "  assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
        "  url = urls[batch_size]\n",
        "  local_file = tf.keras.utils.get_file(os.path.basename(url), origin=url)  \n",
        "  return tf.keras.models.load_model(local_file, compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6y5lNireLWU"
      },
      "source": [
        "Here, to test the pre-trained model, we feed a start string to the model, and get a string as response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # From https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "  num_generate = 200\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(\n",
        "        predictions, num_samples=1)[-1, 0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGAdStJ5wDPV",
        "outputId": "987eb2e2-cfe7-415b-bcec-34596552794b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Text generation requires a batch_size=1 model.\n",
        "keras_model_batch1 = load_model(batch_size=1)\n",
        "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel\n",
            "16195584/16193984 [==============================] - 0s 0us/step\n",
            "What of TensorFlow Federated, you ask? Shall I\n",
            "do well now, I heard her staircases much behind the counter.\n",
            "\n",
            "\"In your arms towards the Notion\n",
            "\n",
            "\n",
            "The hard was demained until at the topic, from turbidly until Younce\n",
            "gentleman, accordin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKMUn-TlgxuP"
      },
      "source": [
        "## Load and Preprocess the Federated Shakespeare Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9vu-GG23LCh"
      },
      "source": [
        "To provide a realistic non-IID data distribution, TFF provides the `tff.simulation.datasets` package. In this package, datasets are split into \"clients\", where each client corresponds to a dataset on a particular device to participate in federated learning.\n",
        "\n",
        "`tff.simulation.datasets.shakespeare.load_data()` returns the train and test Shakespeare federated datasets.\n",
        "\n",
        "The structure of datasets is as follow: The client keys consist of the name of the play joined with the name of the character. For example: `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponds to the lines for the character Othello in the play *Much Ado About Nothing*.\n",
        "\n",
        "Note that in a real federated learning scenario\n",
        "clients are never identified or tracked by ids, but for simulation it is useful to work with keyed datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di3nStTDg0qc",
        "outputId": "61e75ade-7625-49e7-fc5d-108601d54516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
            "1851392/1848122 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-dTfxfREgz9"
      },
      "source": [
        "Here, for example, we can look at some data from King Lear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw5UbwqvEiQS"
      },
      "source": [
        "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
        "raw_example_dataset = train_data.create_tf_dataset_for_client('THE_TRAGEDY_OF_KING_LEAR_KING')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUnbI5Hp4sXg"
      },
      "source": [
        "To prepare the data for training, we use `tf.data.dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W95Of6Bwsrfc"
      },
      "source": [
        "# Input pre-processing parameters\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 5\n",
        "BUFFER_SIZE = 10000  # For dataset shuffling\n",
        "\n",
        "# Construct a lookup table to map string chars to indexes,\n",
        "# using the vocab loaded above:\n",
        "table = tf.lookup.StaticHashTable(\n",
        "  tf.lookup.KeyValueTensorInitializer(\n",
        "    keys=vocab,\n",
        "    values=tf.constant(list(range(len(vocab))), dtype=tf.int64)),\n",
        "  default_value=0)\n",
        "\n",
        "\n",
        "def to_ids(x):\n",
        "  s = tf.reshape(x['snippets'], shape=[1])\n",
        "  chars = tf.strings.bytes_split(s).values\n",
        "  ids = table.lookup(chars)\n",
        "  return ids\n",
        "\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
        "  target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
        "  return (input_text, target_text)\n",
        "\n",
        "\n",
        "def preprocess(dataset):\n",
        "  return (\n",
        "      # Try multiple epochs of local training\n",
        "      dataset.repeat(NUM_EPOCHS)\n",
        "      # Map ASCII chars to int64 indexes using the vocab\n",
        "      .map(to_ids)\n",
        "      # Split into individual chars\n",
        "      .unbatch()\n",
        "      # Form example sequences of SEQ_LENGTH +1\n",
        "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "      # Shuffle and form minibatches\n",
        "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "      # And finally split into (input, target) tuples, each of length SEQ_LENGTH.\n",
        "      .map(split_input_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw98HnKmEhuh"
      },
      "source": [
        "Note that in the formation of the original sequences and in the formation of batches above, we use `drop_remainder=True` for simplicity. This means that characters (clients) that don't have at least `(SEQ_LENGTH + 1) * BATCH_SIZE` chars of text will have empty datasets.\n",
        "\n",
        "Now we can preprocess our `raw_example_dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rTal7bksWwc"
      },
      "source": [
        "example_dataset = preprocess(raw_example_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePT8Oawm8SRP"
      },
      "source": [
        "## Compile the model and test on the preprocessed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEgDsz-48cAq"
      },
      "source": [
        "In order to evaluate our model, we need to compile it with a loss function and metrics.\n",
        "\n",
        "Furthermore, to have a char-level accuracy, we need to define a new metric class. Char-level accuracy is for the fraction of predictions where the highest probability was put on the correct next char."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOUiDBvmWlM9"
      },
      "source": [
        "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
        "\n",
        "  def __init__(self, name='accuracy', dtype=tf.float32):\n",
        "    super().__init__(name, dtype=dtype)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.reshape(y_true, [-1, 1])\n",
        "    y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
        "    return super().update_state(y_true, y_pred, sample_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2X9eFgt94PM"
      },
      "source": [
        "Now we compile a model, and evaluate it on our `example_dataset`. After that, we compare our accuracy to a completely random data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Xd-52-9zGa",
        "outputId": "6a0b694c-c182-47cc-d17e-22bba626fa3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "# Load the model into keras_model\n",
        "keras_model = load_model(batch_size=BATCH_SIZE)\n",
        "\n",
        "# compile our keras_model\n",
        "keras_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[FlattenedCategoricalAccuracy()])\n",
        "\n",
        "# Compute loss and accuracy on an example Shakespeare character\n",
        "loss, accuracy = keras_model.evaluate(example_dataset.take(5), verbose=0)\n",
        "print('Evaluating on an example Shakespeare character:')\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Compare our accuracy to a completely random data\n",
        "random_indexes = np.random.randint(\n",
        "    low=0, high=len(vocab), size=1 * BATCH_SIZE * (SEQ_LENGTH + 1))\n",
        "data = collections.OrderedDict(snippets=tf.constant(\n",
        "    ''.join(np.array(vocab)[random_indexes]), shape=[1, 1]))\n",
        "random_dataset = preprocess(tf.data.Dataset.from_tensor_slices(data))\n",
        "\n",
        "random_guessed_accuracy = 1.0 / len(vocab)\n",
        "print('Expected accuracy for random guessing:', random_guessed_accuracy)\n",
        "\n",
        "loss, accuracy = keras_model.evaluate(random_dataset, steps=10, verbose=0)\n",
        "print('Evaluating on completely random data:', accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating on an example Shakespeare character:\n",
            "Loss: 3.253553628921509\n",
            "Accuracy: 0.41275\n",
            "Expected accuracy for random guessing: 0.011627906976744186\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating on completely random data: 0.012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH0WzL5L8Lm4"
      },
      "source": [
        "## Fine-tune the model with Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCao4M3L_tsA"
      },
      "source": [
        "In order to connect to TFF Core layer, it is necessary to provide a function that TFF can use, so that it can inroduce our model to a graph that the TFF Core controls.\n",
        "\n",
        "To do so, we need to clone our `keras_model` inside a function called `create_tff_model()`, which TFF will call to produce a new copy of the model inside the graph that it will serialize. It is important to construct all the necessary objects we will need inside this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KadIvFp7m6y"
      },
      "source": [
        "def create_tff_model():\n",
        "  # TFF uses a `dummy_batch` so it knows the types and shapes that your model expects.\n",
        "  x = np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH])\n",
        "  dummy_batch = collections.OrderedDict(x=x, y=x)\n",
        "  keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model_clone,\n",
        "      dummy_batch=dummy_batch,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[FlattenedCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJF_yhJxAi2l"
      },
      "source": [
        "Now, everything is ready to construct a `Federated Averaging` iterative process, and fine tune our pre-trained model.\n",
        "\n",
        "Note that, for the reasons that are mentioned and thoroughly explained in the report, we will feed back the final weights to the original Keras model. By doing so, after each round of federated training, we use a compiled Keras model to perform standard evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my3PW3qhAMDA",
        "outputId": "d2dcfe78-e892-4e20-facc-b602a3548b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# This command builds all the TensorFlow graphs and serializes them: \n",
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    model_fn=create_tff_model,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.7),\n",
        "    client_weight_fn=lambda _: tf.constant(1.0),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function zero_all_if_any_non_finite at 0x7f0d30ddeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function zero_all_if_any_non_finite at 0x7f0d30ddeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:8 out of the last 8 calls to <function zero_all_if_any_non_finite at 0x7f0d30ddeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:8 out of the last 8 calls to <function zero_all_if_any_non_finite at 0x7f0d30ddeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CjvVg0FZpS"
      },
      "source": [
        "Now that we have built all the TensorFlow graphs and serialized them, we write our training and evaluation loop. Note that, in order to accelerate our training step, we select three clients, and train our model only on them. In other words, we will overfit our model on these three clients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE386-rbMCve"
      },
      "source": [
        "def data(client, source=train_data):\n",
        "  return preprocess(\n",
        "      source.create_tf_dataset_for_client(client)).take(5)\n",
        "\n",
        "# our three selected clients\n",
        "clients = ['ALL_S_WELL_THAT_ENDS_WELL_CELIA',\n",
        "           'MUCH_ADO_ABOUT_NOTHING_OTHELLO',\n",
        "           'THE_TRAGEDY_OF_KING_LEAR_KING']\n",
        "\n",
        "train_datasets = [data(client) for client in clients]\n",
        "\n",
        "# We concatenate the test datasets for evaluation with Keras.\n",
        "test_dataset = functools.reduce(\n",
        "    lambda d1, d2: d1.concatenate(d2),\n",
        "    [data(client, test_data) for client in clients])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU3FuY00MOoX"
      },
      "source": [
        "For the reason that `clone_model()` does not clone the weights, and since we want to use the weights from the pre-trained model for the initial state of the model which is produced by `fed_avg.initialize()`, we set the model weights in the server state directly from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm_-PU8OFXpY",
        "outputId": "8d359789-d15d-478c-8d2c-9784cfef1537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_ROUNDS = 30\n",
        "\n",
        "# The state of the FL server, containing the model and optimization state.\n",
        "state = fed_avg.initialize()\n",
        "\n",
        "state = tff.learning.state_with_new_model_weights(\n",
        "    state,\n",
        "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
        "    non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights])\n",
        "\n",
        "\n",
        "def keras_evaluate(state, round_num):\n",
        "  # Take our global model weights and push them back into a Keras model to\n",
        "  # use its standard `.evaluate()` method.\n",
        "  keras_model = load_model(batch_size=BATCH_SIZE)\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[FlattenedCategoricalAccuracy()])\n",
        "  tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
        "  loss, accuracy = keras_model.evaluate(example_dataset, steps=2, verbose=0)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))\n",
        "\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "  print('Round {r}'.format(r=round_num + 1))\n",
        "  keras_evaluate(state, round_num)\n",
        "  state, metrics = fed_avg.next(state, train_datasets)\n",
        "  print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(\n",
        "      l=metrics.loss, a=metrics.accuracy))\n",
        "\n",
        "keras_evaluate(state, NUM_ROUNDS + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round 1\n",
            "\tEval: loss=3.085, accuracy=0.417\n",
            "\tTrain: loss=3.138, accuracy=0.420\n",
            "Round 2\n",
            "\tEval: loss=2.825, accuracy=0.453\n",
            "\tTrain: loss=2.700, accuracy=0.460\n",
            "Round 3\n",
            "\tEval: loss=2.394, accuracy=0.460\n",
            "\tTrain: loss=2.430, accuracy=0.477\n",
            "Round 4\n",
            "\tEval: loss=2.046, accuracy=0.525\n",
            "\tTrain: loss=2.219, accuracy=0.512\n",
            "Round 5\n",
            "\tEval: loss=2.036, accuracy=0.520\n",
            "\tTrain: loss=2.148, accuracy=0.511\n",
            "Round 6\n",
            "\tEval: loss=1.783, accuracy=0.576\n",
            "\tTrain: loss=2.045, accuracy=0.528\n",
            "Round 7\n",
            "\tEval: loss=1.714, accuracy=0.575\n",
            "\tTrain: loss=1.950, accuracy=0.537\n",
            "Round 8\n",
            "\tEval: loss=1.686, accuracy=0.570\n",
            "\tTrain: loss=1.858, accuracy=0.562\n",
            "Round 9\n",
            "\tEval: loss=1.571, accuracy=0.588\n",
            "\tTrain: loss=1.726, accuracy=0.576\n",
            "Round 10\n",
            "\tEval: loss=1.635, accuracy=0.596\n",
            "\tTrain: loss=1.701, accuracy=0.577\n",
            "Round 11\n",
            "\tEval: loss=1.546, accuracy=0.611\n",
            "\tTrain: loss=1.619, accuracy=0.592\n",
            "Round 12\n",
            "\tEval: loss=1.350, accuracy=0.640\n",
            "\tTrain: loss=1.506, accuracy=0.619\n",
            "Round 13\n",
            "\tEval: loss=1.346, accuracy=0.676\n",
            "\tTrain: loss=1.461, accuracy=0.619\n",
            "Round 14\n",
            "\tEval: loss=1.221, accuracy=0.693\n",
            "\tTrain: loss=1.542, accuracy=0.606\n",
            "Round 15\n",
            "\tEval: loss=1.235, accuracy=0.684\n",
            "\tTrain: loss=1.422, accuracy=0.627\n",
            "Round 16\n",
            "\tEval: loss=1.173, accuracy=0.691\n",
            "\tTrain: loss=1.316, accuracy=0.663\n",
            "Round 17\n",
            "\tEval: loss=1.025, accuracy=0.731\n",
            "\tTrain: loss=1.242, accuracy=0.680\n",
            "Round 18\n",
            "\tEval: loss=0.952, accuracy=0.764\n",
            "\tTrain: loss=1.241, accuracy=0.680\n",
            "Round 19\n",
            "\tEval: loss=1.103, accuracy=0.709\n",
            "\tTrain: loss=1.229, accuracy=0.688\n",
            "Round 20\n",
            "\tEval: loss=0.900, accuracy=0.772\n",
            "\tTrain: loss=1.192, accuracy=0.694\n",
            "Round 21\n",
            "\tEval: loss=0.960, accuracy=0.759\n",
            "\tTrain: loss=1.205, accuracy=0.691\n",
            "Round 22\n",
            "\tEval: loss=0.896, accuracy=0.769\n",
            "\tTrain: loss=1.119, accuracy=0.707\n",
            "Round 23\n",
            "\tEval: loss=0.832, accuracy=0.801\n",
            "\tTrain: loss=1.077, accuracy=0.726\n",
            "Round 24\n",
            "\tEval: loss=0.839, accuracy=0.808\n",
            "\tTrain: loss=1.033, accuracy=0.738\n",
            "Round 25\n",
            "\tEval: loss=0.724, accuracy=0.827\n",
            "\tTrain: loss=1.039, accuracy=0.742\n",
            "Round 26\n",
            "\tEval: loss=0.728, accuracy=0.829\n",
            "\tTrain: loss=0.937, accuracy=0.758\n",
            "Round 27\n",
            "\tEval: loss=0.729, accuracy=0.826\n",
            "\tTrain: loss=0.964, accuracy=0.756\n",
            "Round 28\n",
            "\tEval: loss=0.634, accuracy=0.845\n",
            "\tTrain: loss=0.929, accuracy=0.765\n",
            "Round 29\n",
            "\tEval: loss=0.749, accuracy=0.827\n",
            "\tTrain: loss=0.937, accuracy=0.764\n",
            "Round 30\n",
            "\tEval: loss=0.687, accuracy=0.852\n",
            "\tTrain: loss=0.912, accuracy=0.776\n",
            "\tEval: loss=0.682, accuracy=0.841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoshvcHhXVa6"
      },
      "source": [
        "We can test our model by calling `generate_text()`, and giving the `keras_model` and a string as inputs. Note that text generation requires `batch_size=1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTUig7QmXavy",
        "outputId": "6c6079a9-943f-4bef-dd13-dfeeb13d3109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Set our newly trained weights back in the originally created model.\n",
        "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
        "# Text generation requires batch_size=1\n",
        "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What of TensorFlow Federated, you ask? Shall I\r\n",
            "\r\n",
            "\"Tell me what is it.\"\r\n",
            "\r\n",
            "\"She had an impact feelly in a traband, and came running at emerge carlied with\r\n",
            "visible besides anything good remembran every head. My mother in the old law, seeme\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I79dBYksDQp"
      },
      "source": [
        "Based on what we have done, we should expect that the generated text should be similar to the data from our three chosen clients. By selecting more clients, training more, and changing the clients between each `process`, we can get better results."
      ]
    }
  ]
}